test
dwqInsights::foresiteR()
test
dwqInsights::foresiteR()
dwqInsights::foresiteR()
wqTools::bu_poly$bu_class[1]
runApp('inst/foresiteR')
runApp('inst/foresiteR')
knitr::opts_chunk$set(echo = TRUE)
library(plyr)
library(dplyr)
library(openxlsx)
# library(tidyverse)
library(dwqInsights)
library(lubridate)
library(plotly)
library(plyr)
library(dplyr)
library(openxlsx)
# library(tidyverse)
library(dwqInsights)
library(lubridate)
library(plotly)
#' Prep data for TMDL analysis and plotting
#'
#' This function takes parameter concentration and flow data (if applicable) to calculate mean concentrations and loadings on a daily basis, with additional information provided for other aggregation exercises. Produces outputs that may be fed into plotting functions within the dwqInsights package.
#' @param dat_path A file path to the .csv file containing parameter and flow data for sites of interest. File has same column names as an EPA Water Quality Portal narrowresult object, with NumericCriterion and BeneficialUse columns added. Note that this function does not handle detection limits. These must be handled prior to calculating loading.
#' @param cf Numeric. The correction factor to be applied to the loading calculation. Ensure this correction factor is in the correct units to accommodate flow and parameter units.
#' @param mos Numeric. A proportion between 0 and 1 to be used as the margin of safety applied to the TMDL calculations. In other words, this proportion describes the % reduction applied to the straight TMDL loading value based on the standard.
#' @param rec_ssn Numeric. A two-object vector of year days signifying the start and end to the rec season.
#' @param irg_ssn Numeric. A two-object vector of year days signifying the start and end to the irrigation season.
#' @param aggFun String. A character object describing the function used to aggregate to daily/monthly/rec season/irrigation season values. Most typically will be one of the following: gmean, mean, max, min.
#' @param exportfromfunc Logical. Indicates whether workbook should be exported from tmdlCalcs function, or skipped if using Shiny interface. Default is FALSE to accommodate Shiny use.
#' @return The output includes a new Excel workbook with the name of the original file plus today's date.xlsx, as well as the following dataframes, composed within a list: ecoli concentrations, flow data, ldc data, monthly means, rec/non rec means, and irg/non irg means.
#' @export tmdlCalcs
#' @import lubridate
#' @import plyr
#' @import dplyr
#' @import shiny
#' @import openxlsx
#' @import data.table
test = tmdlCalcs(wb_path = wb_path, aggFun = "gmean", cf=24465715, mos=0, exportfromfunc = TRUE)
tmdlCalcs <- function(wb_path, aggFun="mean", cf, mos= 0, rec_ssn=c(121,304), irg_ssn=c(135,288), exportfromfunc = FALSE){
warning("This function does not handle special characters, detection limits or fractions and does not calculate correction-factor dependent criteria. Please make these adjustments prior to using tmdlCalcs.")
## Calculation functions needed for plotting and assessment ##
perc.red <- function(x,y){100-x/y*100} # percent reduction equation where x = capacity and y = observed
flow_perc <- function(x){(1-percent_rank(x))*100} # gives each flow measurement a percent rank (what percentage of flows are higher than value?)
if(aggFun=="gmean"){
aggFun = function(x){exp(mean(log(x)))}
}
# Determine calendar season - taken from https://stackoverflow.com/questions/9500114/find-which-season-a-particular-date-belongs-to
getSeason <- function(DATES) {
WS <- as.Date("2012-12-15", format = "%Y-%m-%d") # Winter Solstice
SE <- as.Date("2012-3-15",  format = "%Y-%m-%d") # Spring Equinox
SS <- as.Date("2012-6-15",  format = "%Y-%m-%d") # Summer Solstice
FE <- as.Date("2012-9-15",  format = "%Y-%m-%d") # Fall Equinox
# Convert dates from any year to 2012 dates
d <- as.Date(strftime(DATES, format="2012-%m-%d"))
ifelse (d >= WS | d < SE, "Winter",
ifelse (d >= SE & d < SS, "Spring",
ifelse (d >= SS & d < FE, "Summer", "Fall")))}
# Load csv
param.dat <- read.csv(wb_path)
start = dim(param.dat)[1]
param.dat$Date <- as.Date(param.dat$ActivityStartDate, format="%m/%d/%Y")
param.dat = subset(param.dat, !is.na(as.numeric(param.dat$ResultMeasureValue)))
end = dim(param.dat)[1]
if(!(start==end)){
n = start-end
warning(paste0(n," records removed because result value was non-numeric or NA"))
}
# Show parameters and units to inform user composition of dataset
tbl = unique(param.dat[,c("CharacteristicName","ResultMeasure.MeasureUnitCode")])
tbl$concat = apply(tbl, 1 , paste, collapse = "-" )
readline(paste0("The following parameters are in the dataset: ",paste(tbl$concat, collapse=", "),". Press [enter] to continue or [esc] to exit."))
# Aggregate to daily values
param.agg = subset(param.dat, !param.dat$CharacteristicName%in%c("Flow"))
param.agg.dv = aggregate(ResultMeasureValue~BeneficialUse+MonitoringLocationIdentifier+Date+CharacteristicName+ResultMeasure.MeasureUnitCode+NumericCriterion, data=param.agg, FUN=aggFun)
names(param.agg.dv)[names(param.agg.dv)=="ResultMeasureValue"] = "DailyResultMeasureValue"
param.agg.dv$Exceeds = ifelse(param.agg.dv$DailyResultMeasureValue>param.agg.dv$NumericCriterion,1,0)
param.agg.dv$Season = getSeason(param.agg.dv$Date)
# Aggregate to monthly means
# month.agg = param.agg.dv
# month.agg$month = lubridate::month(month.agg$Date)
# month.agg = aggregate(DailyResultMeasureValue~MonitoringLocationIdentifier+month+CharacteristicName+ResultMeasure.MeasureUnitCode+NumericCriterion, data=month.agg, FUN=aggFun)
# names(month.agg)[names(month.agg)=="DailyResultMeasureValue"] = "MonthResultMeasureValue"
# month.agg$Percent_Reduction = perc.red(month.agg$NumericCriterion, month.agg$MonthResultMeasureValue)
# month.agg$Percent_Reduction[month.agg$Percent_Reduction<0] = 0
# Rec season and Irrigation season
param.agg.dv$Rec_Season = ifelse(lubridate::yday(param.agg.dv$Date)>=rec_ssn[1]&lubridate::yday(param.agg.dv$Date)<=rec_ssn[2],"rec","not rec")
param.agg.dv$Irg_Season = ifelse(lubridate::yday(param.agg.dv$Date)>=irg_ssn[1]&lubridate::yday(param.agg.dv$Date)<=irg_ssn[2],"irrigation","not irrigation")
if("Flow"%in%unique(tbl$CharacteristicName)){
flow.agg = subset(param.dat, param.dat$CharacteristicName%in%c("Flow"))
flow.agg = aggregate(ResultMeasureValue~MonitoringLocationIdentifier+Date+ResultMeasure.MeasureUnitCode, data=flow.agg, FUN="mean")
names(flow.agg)[names(flow.agg)=="ResultMeasureValue"] = "DailyFlowValue"
names(flow.agg)[names(flow.agg)=="ResultMeasure.MeasureUnitCode"] = "FlowUnit"
flow.agg$Flow_Percentile = flow_perc(flow.agg$DailyFlowValue)
param.agg.dv = merge(param.agg.dv, flow.agg, all.x = TRUE)
param.agg.dv$Observed_Loading = param.agg.dv$DailyResultMeasureValue*cf*param.agg.dv$DailyFlowValue
param.agg.dv$TMDL = param.agg.dv$NumericCriterion*cf*param.agg.dv$DailyFlowValue*(1-mos)
}
############################ SAVE WORKBOOK FILE WITH NEW SHEETS #########################
if(exportfromfunc){
write.csv(param.agg.dv,paste0("tmdlCalc_output_",Sys.Date(),".csv"), row.names = FALSE)
}
return(param.agg.dv)
}
file_path = "C:\\Users\\ehinman\\Desktop\\template_csv.csv"
aggFun ="gmean" # geometric mean
correction_factor = 24465715 # correction factor for E. coli
margin_of_safety = 0 # percentage
rec_season=c(121,304) # days of the year
irrigation_season=c(135,288) # days of the year
output = tmdlCalcs(wb_path = file_path, aggFun = aggFun, cf = correction_factor, mos = margin_of_safety, rec_ssn = rec_season, irg_ssn = irrigation_season, exportfromfunc = TRUE)
View(output)
colorz = c("#034963","#0b86a3","#00a1c6","#cde3e2", "#BFD7B5","#D1CAA1","#D1D2F9","#77625C","#EFA9AE") # these are HEX color codes. Feel free to lengthen, shorten, or change the colors based on your dataset
setwd("C:\\Users\\ehinman\\Desktop\\test_markdown")
devtools::document()
devtools::document()
devtools::document()
warnings()
devtools::document()
devtools::install_github("utah-dwq/wqTools")
wqTools::baseMap()
dat = readxl::read_xlsx("C:\\Users\\ehinman\\Desktop\\site_review_file - Copy.xlsx")
dat = wqTools::assignAUs(dat, lat="LatitudeMeasure", long = "LongitudeMeasure")
View(dat)
dat = readxl::read_xlsx("C:\\Users\\ehinman\\Desktop\\site_review_file - Copy.xlsx", sheet = "sites")
dat = wqTools::assignAUs(dat, lat="LatitudeMeasure", long = "LongitudeMeasure")
View(dat)
map = wqTools::buildMap()
map = wqTools::baseMap()
map = map%>%addCircleMarkers(lng=dat$LongitudeMeasure, lat=dat$LatitudeMeasure, popup = dat$MonitoringLocationIdentifier)
map
devtools::install_github("edhinman/dwqInsights")
install.packages(c("broom", "callr", "cluster", "corrr", "devtools", "DT", "dtplyr", "evaluate", "fields", "fontawesome", "forcats", "furrr", "future", "gert", "globals", "googlesheets4", "haven", "hms", "htmltools", "httr", "igraph", "knitr", "MASS", "modelr", "nlme", "parallelly", "pillar", "quantreg", "raster", "readxl", "reprex", "rmarkdown", "roxygen2", "rsample", "rstudioapi", "rvest", "s2", "scales", "shinyWidgets", "spam", "stringi", "stringr", "survival", "tarchetypes", "targets", "terra", "tibble", "tidyverse", "tinytex", "viridisLite", "xfun"))
install.packages(c("broom", "callr", "cluster", "corrr", "devtools", "DT", "dtplyr", "evaluate", "fields", "fontawesome", "forcats", "furrr", "future", "gert", "globals", "googlesheets4", "haven", "hms", "htmltools", "httr", "igraph", "knitr", "MASS", "modelr", "nlme", "parallelly", "pillar", "quantreg", "raster", "readxl", "reprex", "rmarkdown", "roxygen2", "rsample", "rstudioapi", "rvest", "s2", "scales", "shinyWidgets", "spam", "stringi", "stringr", "survival", "tarchetypes", "targets", "terra", "tibble", "tidyverse", "tinytex", "viridisLite", "xfun"))
install.packages(c("broom", "callr", "cluster", "corrr", "devtools", "DT", "dtplyr", "evaluate", "fields", "fontawesome", "forcats", "furrr", "future", "gert", "globals", "googlesheets4", "haven", "hms", "htmltools", "httr", "igraph", "knitr", "MASS", "modelr", "nlme", "parallelly", "pillar", "quantreg", "raster", "readxl", "reprex", "rmarkdown", "roxygen2", "rsample", "rstudioapi", "rvest", "s2", "scales", "shinyWidgets", "spam", "stringi", "stringr", "survival", "tarchetypes", "targets", "terra", "tibble", "tidyverse", "tinytex", "viridisLite", "xfun"))
install.packages(c("broom", "callr", "cluster", "corrr", "devtools", "DT", "dtplyr", "evaluate", "fields", "fontawesome", "forcats", "furrr", "future", "gert", "globals", "googlesheets4", "haven", "hms", "htmltools", "httr", "igraph", "knitr", "MASS", "modelr", "nlme", "parallelly", "pillar", "quantreg", "raster", "readxl", "reprex", "rmarkdown", "roxygen2", "rsample", "rstudioapi", "rvest", "s2", "scales", "shinyWidgets", "spam", "stringi", "stringr", "survival", "tarchetypes", "targets", "terra", "tibble", "tidyverse", "tinytex", "viridisLite", "xfun"))
devtools::install_github("edhinman/dwqInsights")
dwqInsights::foresiteR()
dat = dwqInsights::tmdlCalcs(wb_path = "C:\\Users\\ehinman\\Desktop\\test_markdown\\template_csv.csv", cf = 24465715, mos=0.1,aggFun="gmean", exportfromfunc = TRUE)
View(dat)
remotes::install_github("USEPA/TADA")
knitr::opts_chunk$set(echo = TRUE)
list.of.packages <- c("plyr","dplyr","openxlsx","ggplot2","tidyr","lubridate","plotly","devtools")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
installed.packages()
installed.packages()[,"Package"]
if(!("dwqInsights"%in%installed.packages()[,"Package"])) devtools::install_github("edhinman/dwqInsights")
remotes::install_github("USGS-R/dataRetrieval", dependencies=TRUE)
library(TADA)
library(dataRetrieval)
vignette(TADA)
vignette("TADA")
vignette("WQPDataHarmonization", package="TADA")
vignette("WQPDataHarmonization.Rmd", package="TADA")
vignette("WQP Data Harmonization", package="TADA")
?vignette
vignette(package="TADA")
wqTools::baseMap()
dwqInsights::foresiteR()
wqTools::baseMap()
knitr::opts_chunk$set(echo = TRUE)
file_path = "Em Creek E coli Data Raw 082922.xlsx - Sheet1.csv"
au_name = "Emigration"
aggFun ="gmean" # geometric mean
correction_factor = 24465715 # correction factor for E. coli
margin_of_safety = 0.1 # percentage
rec_season=c(121,304) # days of the year
irrigation_season=c(135,288) # days of the year
file_path = "em_creek.csv"
au_name = "Emigration"
aggFun ="gmean" # geometric mean
correction_factor = 24465715 # correction factor for E. coli
margin_of_safety = 0.1 # percentage
rec_season=c(121,304) # days of the year
irrigation_season=c(135,288) # days of the year
site_order = data.frame(MonitoringLocationIdentifier = c(
"BR_12.29",
"BR_14.15",
"BR_14.44",
"EM_01.62",
"EM_02.54",
"EM_03.67",
"EM_04.17",
"EM_05.17",
"EM_05.70",
"EM_06.36",
"EM_07.30",
"EM_07.79",
"EM_08.50",
"EM_08.83",
"EM_08.93",
"EM_09.48",
"EM_10.43",
"EM_11.87",
"KL_00.21",
"KL_01.50"
),Order = c(1:20))
colorz = c("#f44336",
"#e81e63",
"#9c27b0",
"#673ab7",
"#3f51b5",
"#2196f3",
"#03a9f4",
"#00bcd4",
"#009688",
"#4caf50",
"#8bc34a",
"#cddc39",
"#ffeb3b",
"#ffc107",
"#ff9800",
"#ff5722",
"#795548",
"#9e9e9e",
"#607d8b",
"#000000")
list.of.packages <- c("plyr","dplyr","openxlsx","ggplot2","tidyr","lubridate","plotly","devtools")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
if(!("dwqInsights"%in%installed.packages()[,"Package"])) devtools::install_github("edhinman/dwqInsights")
library(plyr)
library(dplyr)
library(openxlsx)
library(tidyr)
library(lubridate)
library(plotly)
library(dwqInsights)
colorsy=colorz[1:nrow(site_order)]
source("tmdlCalcs.R")
source("R/tmdlCalcs.R")
output = tmdlCalcs(wb_path = file_path, aggFun = aggFun, cf = correction_factor, mos = margin_of_safety, rec_ssn = rec_season, irg_ssn = irrigation_season, exportfromfunc = TRUE)
output = tmdlCalcs(wb_path = file_path, aggFun = aggFun, cf = correction_factor, mos = margin_of_safety, rec_ssn = rec_season, irg_ssn = irrigation_season, exportfromfunc = TRUE)
output = tmdlCalcs(wb_path = file_path, aggFun = aggFun, cf = correction_factor, mos = margin_of_safety, rec_ssn = rec_season, irg_ssn = irrigation_season, exportfromfunc = TRUE)
output = tmdlCalcs(wb_path = file_path, aggFun = aggFun, cf = correction_factor, mos = margin_of_safety, rec_ssn = rec_season, irg_ssn = irrigation_season, exportfromfunc = TRUE)
wb_path = file_path
aggFun = aggFun
cf = correction_factor
mos = margin_of_safety
rec_ssn = rec_season
irg_ssn = irrigation_season
exportfromfunc = TRUE
## Calculation functions needed for plotting and assessment ##
perc.red <- function(x,y){100-x/y*100} # percent reduction equation where x = capacity and y = observed
flow_perc <- function(x){(1-percent_rank(x))*100} # gives each flow measurement a percent rank (what percentage of flows are higher than value?)
if(aggFun=="gmean"){
aggFun = function(x){exp(mean(log(x)))}
}
# Determine calendar season - taken from https://stackoverflow.com/questions/9500114/find-which-season-a-particular-date-belongs-to
getSeason <- function(DATES) {
WS <- as.Date("2012-12-15", format = "%Y-%m-%d") # Winter Solstice
SE <- as.Date("2012-3-15",  format = "%Y-%m-%d") # Spring Equinox
SS <- as.Date("2012-6-15",  format = "%Y-%m-%d") # Summer Solstice
FE <- as.Date("2012-9-15",  format = "%Y-%m-%d") # Fall Equinox
# Convert dates from any year to 2012 dates
d <- as.Date(strftime(DATES, format="2012-%m-%d"))
ifelse (d >= WS | d < SE, "Winter",
ifelse (d >= SE & d < SS, "Spring",
ifelse (d >= SS & d < FE, "Summer", "Fall")))}
# Load csv
param.dat <- read.csv(wb_path)
start = dim(param.dat)[1]
param.dat$Date <- as.Date(param.dat$ActivityStartDate, format="%m/%d/%Y")
param.dat = subset(param.dat, !is.na(as.numeric(param.dat$ResultMeasureValue)))
end = dim(param.dat)[1]
if(!(start==end)){
n = start-end
warning(paste0(n," records removed because result value was non-numeric or NA"))
}
# Show parameters and units to inform user composition of dataset
tbl = unique(param.dat[,c("CharacteristicName","ResultMeasure.MeasureUnitCode")])
View(param.dat)
ds_names = names(param.dat)
req_names = c("MonitoringLocationIdentifier","ActivityStartDate","CharacteristicName","ResultMeasureValue","ResultMeasure.MeasureUnitCode","BeneficialUse","NumericCriterion")
missing = req_names[!req_names%in%ds_names]
length(new.packages)
source("R/tmdlCalcs.R")
output = tmdlCalcs(wb_path = file_path, aggFun = aggFun, cf = correction_factor, mos = margin_of_safety, rec_ssn = rec_season, irg_ssn = irrigation_season, exportfromfunc = TRUE)
# Load csv
param.dat <- read.csv(wb_path)
ds_names = names(param.dat)
req_names = c("MonitoringLocationIdentifier","ActivityStartDate","CharacteristicName","ResultMeasureValue","ResultMeasure.MeasureUnitCode","BeneficialUse","NumericCriterion")
missing = req_names[!req_names%in%ds_names]
if(length(missing)>0)stop(paste0(missing,"column(s) missing from input dataset. Add required columns before running."))
if(length(missing)>0)stop(paste0(missing," column(s) missing from input dataset. Add required column(s) before running."))
text = paste(missing,collapse=", ")
stop(paste0(missing," column(s) missing from input dataset. Add required column(s) before running."))}
if(length(missing)>0){
text = paste(missing,collapse=", ")
stop(paste0(text," column(s) missing from input dataset. Add required column(s) before running."))
}
#' Prep data for TMDL analysis and plotting
#'
#' This function takes parameter concentration and flow data (if applicable) to calculate mean concentrations and loadings on a daily basis, with additional information provided for other aggregation exercises. Produces outputs that may be fed into plotting functions within the dwqInsights package.
#' @param dat_path A file path to the .csv file containing parameter and flow data for sites of interest. File has same column names as an EPA Water Quality Portal narrowresult object, with NumericCriterion and BeneficialUse columns added. Note that this function does not handle detection limits. These must be handled prior to calculating loading.
#' @param cf Numeric. The correction factor to be applied to the loading calculation. Ensure this correction factor is in the correct units to accommodate flow and parameter units.
#' @param mos Numeric. A proportion between 0 and 1 to be used as the margin of safety applied to the TMDL calculations. In other words, this proportion describes the % reduction applied to the straight TMDL loading value based on the standard.
#' @param rec_ssn Numeric. A two-object vector of year days signifying the start and end to the rec season.
#' @param irg_ssn Numeric. A two-object vector of year days signifying the start and end to the irrigation season.
#' @param aggFun String. A character object describing the function used to aggregate to daily/monthly/rec season/irrigation season values. Most typically will be one of the following: gmean, mean, max, min.
#' @param exportfromfunc Logical. Indicates whether workbook should be exported from tmdlCalcs function, or skipped if using Shiny interface. Default is FALSE to accommodate Shiny use.
#' @return The output includes a new Excel workbook with the name of the original file plus today's date.xlsx, as well as the following dataframes, composed within a list: ecoli concentrations, flow data, ldc data, monthly means, rec/non rec means, and irg/non irg means.
#' @export
#' @import lubridate
#' @import plyr
#' @import dplyr
#' @import shiny
#' @import openxlsx
#' @import data.table
# test = tmdlCalcs(wb_path = wb_path, aggFun = "gmean", cf=24465715, mos=0, exportfromfunc = TRUE)
tmdlCalcs <- function(wb_path, aggFun="mean", cf, mos= 0, rec_ssn=c(121,304), irg_ssn=c(135,288), exportfromfunc = FALSE){
warning("This function does not handle special characters, detection limits or fractions and does not calculate correction-factor dependent criteria. Please make these adjustments prior to using tmdlCalcs.")
## Calculation functions needed for plotting and assessment ##
perc.red <- function(x,y){100-x/y*100} # percent reduction equation where x = capacity and y = observed
flow_perc <- function(x){(1-percent_rank(x))*100} # gives each flow measurement a percent rank (what percentage of flows are higher than value?)
if(aggFun=="gmean"){
aggFun = function(x){exp(mean(log(x)))}
}
# Determine calendar season - taken from https://stackoverflow.com/questions/9500114/find-which-season-a-particular-date-belongs-to
getSeason <- function(DATES) {
WS <- as.Date("2012-12-15", format = "%Y-%m-%d") # Winter Solstice
SE <- as.Date("2012-3-15",  format = "%Y-%m-%d") # Spring Equinox
SS <- as.Date("2012-6-15",  format = "%Y-%m-%d") # Summer Solstice
FE <- as.Date("2012-9-15",  format = "%Y-%m-%d") # Fall Equinox
# Convert dates from any year to 2012 dates
d <- as.Date(strftime(DATES, format="2012-%m-%d"))
ifelse (d >= WS | d < SE, "Winter",
ifelse (d >= SE & d < SS, "Spring",
ifelse (d >= SS & d < FE, "Summer", "Fall")))}
# Load csv
param.dat <- read.csv(wb_path)
ds_names = names(param.dat)
req_names = c("MonitoringLocationIdentifier","ActivityStartDate","CharacteristicName","ResultMeasureValue","ResultMeasure.MeasureUnitCode","BeneficialUse","NumericCriterion")
missing = req_names[!req_names%in%ds_names]
if(length(missing)>0){
text = paste(missing,collapse=", ")
stop(paste0(text," column(s) missing from input dataset. Add required column(s) before running."))
}
start = dim(param.dat)[1]
param.dat$Date <- as.Date(param.dat$ActivityStartDate, format="%m/%d/%Y")
param.dat = subset(param.dat, !is.na(as.numeric(param.dat$ResultMeasureValue)))
end = dim(param.dat)[1]
if(!(start==end)){
n = start-end
warning(paste0(n," records removed because result value was non-numeric or NA"))
}
# Show parameters and units to inform user composition of dataset
tbl = unique(param.dat[,c("CharacteristicName","ResultMeasure.MeasureUnitCode")])
tbl$concat = apply(tbl, 1 , paste, collapse = "-" )
paste0("The following parameters are in the dataset: ",paste(tbl$concat, collapse=", "),". Press [enter] to continue or [esc] to exit.")
# Aggregate to daily values
param.agg = subset(param.dat, !param.dat$CharacteristicName%in%c("Flow"))
param.agg.dv = aggregate(ResultMeasureValue~BeneficialUse+MonitoringLocationIdentifier+Date+CharacteristicName+ResultMeasure.MeasureUnitCode+NumericCriterion, data=param.agg, FUN=aggFun)
names(param.agg.dv)[names(param.agg.dv)=="ResultMeasureValue"] = "DailyResultMeasureValue"
param.agg.dv$Exceeds = ifelse(param.agg.dv$DailyResultMeasureValue>param.agg.dv$NumericCriterion,1,0)
param.agg.dv$Season = getSeason(param.agg.dv$Date)
# Aggregate to monthly means
# month.agg = param.agg.dv
# month.agg$month = lubridate::month(month.agg$Date)
# month.agg = aggregate(DailyResultMeasureValue~MonitoringLocationIdentifier+month+CharacteristicName+ResultMeasure.MeasureUnitCode+NumericCriterion, data=month.agg, FUN=aggFun)
# names(month.agg)[names(month.agg)=="DailyResultMeasureValue"] = "MonthResultMeasureValue"
# month.agg$Percent_Reduction = perc.red(month.agg$NumericCriterion, month.agg$MonthResultMeasureValue)
# month.agg$Percent_Reduction[month.agg$Percent_Reduction<0] = 0
# Rec season and Irrigation season
param.agg.dv$Rec_Season = ifelse(lubridate::yday(param.agg.dv$Date)>=rec_ssn[1]&lubridate::yday(param.agg.dv$Date)<=rec_ssn[2],"rec","not rec")
param.agg.dv$Irg_Season = ifelse(lubridate::yday(param.agg.dv$Date)>=irg_ssn[1]&lubridate::yday(param.agg.dv$Date)<=irg_ssn[2],"irrigation","not irrigation")
if("Flow"%in%unique(tbl$CharacteristicName)){
flow.agg = subset(param.dat, param.dat$CharacteristicName%in%c("Flow"))
flow.agg = aggregate(ResultMeasureValue~MonitoringLocationIdentifier+Date+ResultMeasure.MeasureUnitCode, data=flow.agg, FUN="mean")
names(flow.agg)[names(flow.agg)=="ResultMeasureValue"] = "DailyFlowValue"
names(flow.agg)[names(flow.agg)=="ResultMeasure.MeasureUnitCode"] = "FlowUnit"
flow.agg$Flow_Percentile = flow_perc(flow.agg$DailyFlowValue)
param.agg.dv = merge(param.agg.dv, flow.agg, all.x = TRUE)
param.agg.dv$Observed_Loading = param.agg.dv$DailyResultMeasureValue*cf*param.agg.dv$DailyFlowValue
param.agg.dv$TMDL = param.agg.dv$NumericCriterion*cf*param.agg.dv$DailyFlowValue*(1-mos)
}
############################ SAVE WORKBOOK FILE WITH NEW SHEETS #########################
if(exportfromfunc){
write.csv(param.agg.dv,paste0("tmdlCalc_output_",Sys.Date(),".csv"), row.names = FALSE)
}
return(param.agg.dv)
}
output = tmdlCalcs(wb_path = file_path, aggFun = aggFun, cf = correction_factor, mos = margin_of_safety, rec_ssn = rec_season, irg_ssn = irrigation_season, exportfromfunc = TRUE)
aggFun()
rm(aggFun())
rm(aggFun)
output = tmdlCalcs(wb_path = file_path, aggFun = aggFun, cf = correction_factor, mos = margin_of_safety, rec_ssn = rec_season, irg_ssn = irrigation_season, exportfromfunc = TRUE)
aggFun ="gmean" # geometric mean
output = tmdlCalcs(wb_path = file_path, aggFun = aggFun, cf = correction_factor, mos = margin_of_safety, rec_ssn = rec_season, irg_ssn = irrigation_season, exportfromfunc = TRUE)
output = tmdlCalcs(wb_path = file_path, aggFun = aggFun, cf = correction_factor, mos = margin_of_safety, rec_ssn = rec_season, irg_ssn = irrigation_season, exportfromfunc = TRUE)
head(output)
gmean = function(x){exp(mean(log(x)))}
dat = output
dat_agg = output%>%group_by(MonitoringLocationIdentifier, CharacteristicName,ResultMeasure.MeasureUnitCode)%>%summarise(daterange = paste0(min(Date)," to ",max(Date)),samplesize=length(DailyResultMeasureValue),minconc = min(DailyResultMeasureValue),arithmean=mean(DailyResultMeasureValue),maxconc = max(DailyResultMeasureValue), perc_exc = sum(Exceeds)/length(Exceeds)*100)
rec_mean = output%>%filter(Rec_Season=="rec")%>%group_by(MonitoringLocationIdentifier, CharacteristicName,ResultMeasure.MeasureUnitCode)%>%summarise(rec_ssn_arithmean=mean(DailyResultMeasureValue),rec_ssn_perc_exc = sum(Exceeds)/length(Exceeds)*100)
dat_agg = merge(dat_agg, rec_mean, all = TRUE)
if(aggFun=="gmean"){
dat_agg_geo = output%>%group_by(MonitoringLocationIdentifier, CharacteristicName, ResultMeasure.MeasureUnitCode)%>%summarise(geomean = gmean(DailyResultMeasureValue))
rec_geo = output%>%filter(Rec_Season=="rec")%>%group_by(MonitoringLocationIdentifier, CharacteristicName, ResultMeasure.MeasureUnitCode)%>%summarise(rec_ssn_geomean=gmean(DailyResultMeasureValue))
dat_agg_geo = merge(dat_agg_geo, rec_geo, all = TRUE)
}
dat_agg = merge(dat_agg, dat_agg_geo, all = TRUE)
if("DailyFlowValue"%in%colnames(output)){
flo_agg = output%>%filter(!is.na(DailyFlowValue))%>%group_by(MonitoringLocationIdentifier, ResultMeasure.MeasureUnitCode)%>%summarise(daterange = paste0(min(Date)," to ",max(Date)),samplesize=length(DailyFlowValue),minconc = min(DailyFlowValue),arithmean=mean(DailyFlowValue),maxconc = max(DailyFlowValue))
dat_agg = plyr::rbind.fill(dat_agg, flo_agg)
dat_agg$CharacteristicName[is.na(dat_agg$CharacteristicName)] = "Flow"
}
write.csv(dat_agg, paste0(au_name,"_summary_table_stats.csv"), row.names = FALSE)
head(dat_agg)
## GGPLOT
sites = site_order[order(site_order$Order),]
sites = as.character(sites$MonitoringLocationIdentifier)
dat = output
dat$MonitoringLocationIdentifier = factor(as.character(dat$MonitoringLocationIdentifier), levels = sites)
f = ggplot(dat, aes(x=MonitoringLocationIdentifier,y=DailyResultMeasureValue))+geom_jitter(aes(fill=factor(MonitoringLocationIdentifier)), position=position_jitter(0.1), shape=21, size=2, color="#646464")+scale_fill_manual(values= colorsy)+theme_classic()+labs(x="Monitoring Location ID", y=unique(dat$ResultMeasure.MeasureUnitCode))+stat_summary(fun=gmean, geom="point", fill="#FFB800",color="black", size=2.5, shape=23)+geom_hline(yintercept=unique(dat$NumericCriterion), color="#cb181d",size=0.5)+theme(legend.position = "none") # NOTE, should adjust numeric criterion line to accommodate variable criteria (e.g. hardness-dependent)
f
ggsave(paste0(au_name,"_downstream_upstream.jpg"),width=6,height=4, units="in", dpi=500)
dat = output
dat = dat[order(dat$Date),]
dat$MonitoringLocationIdentifier = factor(as.character(dat$MonitoringLocationIdentifier), levels = sites)
t = ggplot(dat, aes(Date,DailyResultMeasureValue, fill=as.factor(MonitoringLocationIdentifier)))+scale_x_date(limits=c(min(dat$Date), max(dat$Date)))+geom_point(shape=21,color="#646464", size=2)+facet_wrap(vars(MonitoringLocationIdentifier), scales="free", ncol=2)+theme_classic()+labs(x="Date", y=unique(dat$ResultMeasure.MeasureUnitCode))+geom_hline(yintercept=unique(dat$NumericCriterion), color="#cb181d",size=0.5)+theme(legend.position = "none")+scale_fill_manual(values=colorsy)
t
ggsave(paste0(au_name,"_timeseries.jpg"),width=6,height=5, units="in", dpi=500)
dat = output
dat$month = lubridate::month(dat$Date, label=TRUE)
dat = dat[order(dat$month),]
dat$MonitoringLocationIdentifier = factor(as.character(dat$MonitoringLocationIdentifier), levels = sites)
g = ggplot(dat, aes(month, DailyResultMeasureValue, fill=factor(MonitoringLocationIdentifier)))+geom_blank()+geom_rect(aes(xmin=4.5,ymin=0,xmax=10.5,ymax=max(DailyResultMeasureValue)), fill="#D3D3D3")+scale_x_discrete(limits=month.abb)+geom_jitter(position=position_jitter(0), size=3, shape=21, color="#646464", alpha=0.65)+facet_wrap(vars(MonitoringLocationIdentifier), scales="free", ncol=2)+theme_classic()+labs(x="Month", y=unique(dat$ResultMeasure.MeasureUnitCode))+geom_hline(yintercept=unique(dat$NumericCriterion), color="#cb181d",size=0.5)+theme(legend.position = "none")+scale_fill_manual(values=colorsy)+stat_summary(fun=gmean, geom="point", fill="#FFB800",color="black", size=3, shape=23)
g
ggsave(paste0(au_name,"_monthly.jpg"),width=8,height=5, units="in", dpi=500)
if("TMDL"%in%colnames(output)){
ldc=output
ldc_month = subset(ldc, !is.na(ldc$Observed_Loading))
ldc_month = ldc_month%>%select(Date,MonitoringLocationIdentifier,TMDL,Observed_Loading)%>%pivot_longer(cols=c(TMDL,Observed_Loading),names_to="Type",values_to="Loading",values_drop_na=TRUE)
ldc_month$Loading_Giga = ldc_month$Loading/1000000000
ldc_month$month = lubridate::month(ldc_month$Date, label=TRUE, abbr=TRUE)
ldc_month1 = ldc_month%>%group_by(MonitoringLocationIdentifier,Type,month)%>%summarise(mean_Load = mean(Loading_Giga))
ldcsites = unique(ldc_month1$MonitoringLocationIdentifier)
for(i in 1:length(ldcsites)){
name = ldcsites[i]
g = ggplot(ldc_month1, aes(x=month, y=mean_Load, fill=Type))+geom_blank()+theme_classic()+geom_rect(aes(xmin=4.5,ymin=0,xmax=10.5,ymax=max(ldc_month1$mean_Load*1.1)), fill="#D3D3D3")+scale_x_discrete(limits=month.abb)+labs(x="Month",y="GigaMPN/day")+geom_col(position="dodge", color="#646464")+scale_fill_manual(values=c("#00a1c6","#034963"),name="",breaks=c("Observed_Loading","TMDL"),labels=c("Observed","TMDL"))+guides(color="none")
g
ggsave(paste0(name,"_monthly_load.jpg"),width=8,height=4, units="in", dpi=500)
}
g
}
dat = output
dat$month = lubridate::month(dat$Date, label=TRUE)
dat = dat[order(dat$month),]
dat$MonitoringLocationIdentifier = factor(as.character(dat$MonitoringLocationIdentifier), levels = sites)
g = ggplot(dat, aes(month, DailyResultMeasureValue, fill=factor(MonitoringLocationIdentifier)))+geom_blank()+geom_rect(aes(xmin=4.5,ymin=0,xmax=10.5,ymax=max(DailyResultMeasureValue)), fill="#D3D3D3")+scale_x_discrete(limits=month.abb)+geom_jitter(position=position_jitter(0), size=3, shape=21, color="#646464", alpha=0.65)+facet_wrap(vars(MonitoringLocationIdentifier), scales="free", ncol=1)+theme_classic()+labs(x="Month", y=unique(dat$ResultMeasure.MeasureUnitCode))+geom_hline(yintercept=unique(dat$NumericCriterion), color="#cb181d",size=0.5)+theme(legend.position = "none")+scale_fill_manual(values=colorsy)+stat_summary(fun=gmean, geom="point", fill="#FFB800",color="black", size=3, shape=23)
g
ggsave(paste0(au_name,"_monthly.jpg"),width=8,height=5, units="in", dpi=500)
ggsave(paste0(au_name,"_monthly.jpg"),width=8,height=16, units="in", dpi=500)
## GGPLOT
sites = site_order[order(site_order$Order),]
sites = as.character(sites$MonitoringLocationIdentifier)
dat = output
dat$MonitoringLocationIdentifier = factor(as.character(dat$MonitoringLocationIdentifier), levels = sites)
f = ggplot(dat, aes(x=MonitoringLocationIdentifier,y=DailyResultMeasureValue))+geom_jitter(aes(fill=factor(MonitoringLocationIdentifier)), position=position_jitter(0.1), shape=21, size=2, color="#646464")+scale_fill_manual(values= colorsy)+theme_classic()+labs(x="Monitoring Location ID", y=unique(dat$ResultMeasure.MeasureUnitCode))+stat_summary(fun=gmean, geom="point", fill="#FFB800",color="black", size=2.5, shape=23)+geom_hline(yintercept=unique(dat$NumericCriterion), color="#cb181d",size=0.5)+theme(legend.position = "none", axis.text.x=element_text(angle=-45, vjust=1, hjust=1)) # NOTE, should adjust numeric criterion line to accommodate variable criteria (e.g. hardness-dependent)
f
ggsave(paste0(au_name,"_downstream_upstream.jpg"),width=6,height=4, units="in", dpi=500)
## GGPLOT
sites = site_order[order(site_order$Order),]
sites = as.character(sites$MonitoringLocationIdentifier)
dat = output
dat$MonitoringLocationIdentifier = factor(as.character(dat$MonitoringLocationIdentifier), levels = sites)
f = ggplot(dat, aes(x=MonitoringLocationIdentifier,y=DailyResultMeasureValue))+geom_jitter(aes(fill=factor(MonitoringLocationIdentifier)), position=position_jitter(0.1), shape=21, size=2, color="#646464")+scale_fill_manual(values= colorsy)+theme_classic()+labs(x="Monitoring Location ID", y=unique(dat$ResultMeasure.MeasureUnitCode))+stat_summary(fun=gmean, geom="point", fill="#FFB800",color="black", size=2.5, shape=23)+geom_hline(yintercept=unique(dat$NumericCriterion), color="#cb181d",size=0.5)+theme(legend.position = "none", axis.text.x=element_text(angle=-45)) # NOTE, should adjust numeric criterion line to accommodate variable criteria (e.g. hardness-dependent)
f
ggsave(paste0(au_name,"_downstream_upstream.jpg"),width=6,height=4, units="in", dpi=500)
View(dat_agg)
unique(dat_agg$MonitoringLocationIdentifier)
## GGPLOT
sites = site_order[order(site_order$Order),]
sites = as.character(sites$MonitoringLocationIdentifier)
dat = output
dat$MonitoringLocationIdentifier = factor(as.character(dat$MonitoringLocationIdentifier), levels = sites)
f = ggplot(dat, aes(x=MonitoringLocationIdentifier,y=DailyResultMeasureValue))+geom_jitter(aes(fill=factor(MonitoringLocationIdentifier)), position=position_jitter(0.1), shape=21, size=2, color="#646464")+scale_fill_manual(values= colorsy)+theme_classic()+labs(x="Monitoring Location ID", y=unique(dat$ResultMeasure.MeasureUnitCode))+stat_summary(fun=gmean, geom="point", fill="#FFB800",color="black", size=2.5, shape=23)+geom_hline(yintercept=unique(dat$NumericCriterion), color="#cb181d",size=0.5)+theme(legend.position = "none", axis.text.x=element_text(angle=-45, hjust=1)) # NOTE, should adjust numeric criterion line to accommodate variable criteria (e.g. hardness-dependent)
f
ggsave(paste0(au_name,"_downstream_upstream.jpg"),width=6,height=4, units="in", dpi=500)
## GGPLOT
sites = site_order[order(site_order$Order),]
sites = as.character(sites$MonitoringLocationIdentifier)
dat = output
dat$MonitoringLocationIdentifier = factor(as.character(dat$MonitoringLocationIdentifier), levels = sites)
f = ggplot(dat, aes(x=MonitoringLocationIdentifier,y=DailyResultMeasureValue))+geom_jitter(aes(fill=factor(MonitoringLocationIdentifier)), position=position_jitter(0.1), shape=21, size=2, color="#646464")+scale_fill_manual(values= colorsy)+theme_classic()+labs(x="Monitoring Location ID", y=unique(dat$ResultMeasure.MeasureUnitCode))+stat_summary(fun=gmean, geom="point", fill="#FFB800",color="black", size=2.5, shape=23)+geom_hline(yintercept=unique(dat$NumericCriterion), color="#cb181d",size=0.5)+theme(legend.position = "none", axis.text.x=element_text(angle=45, hjust=1)) # NOTE, should adjust numeric criterion line to accommodate variable criteria (e.g. hardness-dependent)
f
ggsave(paste0(au_name,"_downstream_upstream.jpg"),width=6,height=4, units="in", dpi=500)
?facet_wrap
dat = output
dat = dat[order(dat$Date),]
dat$MonitoringLocationIdentifier = factor(as.character(dat$MonitoringLocationIdentifier), levels = sites)
t = ggplot(dat, aes(Date,DailyResultMeasureValue, fill=as.factor(MonitoringLocationIdentifier)))+scale_x_date(limits=c(min(dat$Date), max(dat$Date)))+geom_point(shape=21,color="#646464", size=2)+facet_wrap(vars(MonitoringLocationIdentifier), scales="free", ncol=1)+theme_classic()+labs(x="Date", y=unique(dat$ResultMeasure.MeasureUnitCode))+geom_hline(yintercept=unique(dat$NumericCriterion), color="#cb181d",size=0.5)+theme(legend.position = "none")+scale_fill_manual(values=colorsy)
t
ggsave(paste0(au_name,"_timeseries.jpg"),width=6,height=5, units="in", dpi=500)
dat = output
dat = dat[order(dat$Date),]
dat$MonitoringLocationIdentifier = factor(as.character(dat$MonitoringLocationIdentifier), levels = sites)
t = ggplot(dat, aes(Date,DailyResultMeasureValue, fill=as.factor(MonitoringLocationIdentifier)))+scale_x_date(limits=c(min(dat$Date), max(dat$Date)))+geom_point(shape=21,color="#646464", size=2)+facet_wrap(vars(MonitoringLocationIdentifier), scales="free", ncol=1)+theme_classic()+labs(x="Date", y=unique(dat$ResultMeasure.MeasureUnitCode))+geom_hline(yintercept=unique(dat$NumericCriterion), color="#cb181d",size=0.5)+theme(legend.position = "none")+scale_fill_manual(values=colorsy)
t
ggsave(paste0(au_name,"_timeseries.jpg"),width=6,height=5, units="in", dpi=500)
dat = output
dat$month = lubridate::month(dat$Date, label=TRUE)
dat = dat[order(dat$month),]
dat$MonitoringLocationIdentifier = factor(as.character(dat$MonitoringLocationIdentifier), levels = sites)
g = ggplot(dat, aes(month, DailyResultMeasureValue, fill=factor(MonitoringLocationIdentifier)))+geom_blank()+geom_rect(aes(xmin=4.5,ymin=0,xmax=10.5,ymax=max(DailyResultMeasureValue)), fill="#D3D3D3")+scale_x_discrete(limits=month.abb)+geom_jitter(position=position_jitter(0), size=3, shape=21, color="#646464", alpha=0.65)+facet_wrap(vars(MonitoringLocationIdentifier), scales="free", ncol=1)+theme_classic()+labs(x="Month", y=unique(dat$ResultMeasure.MeasureUnitCode))+geom_hline(yintercept=unique(dat$NumericCriterion), color="#cb181d",size=0.5)+theme(legend.position = "none")+scale_fill_manual(values=colorsy)+stat_summary(fun=gmean, geom="point", fill="#FFB800",color="black", size=3, shape=23)
g
ggsave(paste0(au_name,"_monthly.jpg"),width=8,height=5, units="in", dpi=500)
